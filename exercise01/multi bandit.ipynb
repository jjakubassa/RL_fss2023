{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just some thoughts i had in the exercise:\n",
    "* rewards are stochastic\n",
    "* possible approach: continuously sample arms and then progressively eliminate actions with lowest expected return\n",
    "* maybe apply epsilon-greedy action selection (see decision support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_bandits\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.basics import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditGame:\n",
    "    def run_n_episodes(self, strategy, n_episodes: int, verbose: bool, strategy_args = []):\n",
    "        # Setup run\n",
    "        # np.random.seed(42)  # make runs deterministic for numpy random number generator\n",
    "        self.env = gym.make(\"BanditTenArmedGaussian-v0\")\n",
    "        # self.env.seed(34)  # make each run the same\n",
    "        self.observation = self.env.reset()\n",
    "        self.rewards = []\n",
    "        self.average_rewards = np.zeros(self.env.action_space.n)\n",
    "        self.nr_steps_per_action = np.zeros(self.env.action_space.n)\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "        if verbose:\n",
    "            print(\"observation space:\", self.env.observation_space.n, \"dimensional\")\n",
    "            print(\"action space:\", self.env.action_space.n, \"dimensional\")\n",
    "\n",
    "        for self.i_episode in range(self.n_episodes):\n",
    "\n",
    "            if verbose:\n",
    "                print(\"episode Number is\", self.i_episode)\n",
    "\n",
    "            action = strategy(*strategy_args)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"action is\", action)\n",
    "\n",
    "            self.observation, self.reward, self.done, self.info = self.env.step(action)\n",
    "            self.rewards.append(self.reward)\n",
    "            self.average_rewards[action] += self.reward / self.n_episodes\n",
    "\n",
    "            if verbose:\n",
    "                print(\"observation space is: \", self.observation)\n",
    "                print(\"reward variable is: \", self.reward)\n",
    "                print(\"done flag is: \", self.done)\n",
    "                print(\"info variable is: \", self.info)\n",
    "        if verbose:\n",
    "            print(\"sum of rewards: \" + str(np.sum(self.rewards)))\n",
    "        return np.sum(self.rewards)\n",
    "    \n",
    "    def run_n_sims(self, strategy, n_sims, n_episodes: int, verbose: bool, strategy_args = []):\n",
    "        self.reward_sims = np.zeros(n_sims)\n",
    "\n",
    "        # make strategies comparable\n",
    "        # np.random.seed(42)\n",
    "        # self.env.seed(34)  \n",
    "\n",
    "        for i_sims in range(n_sims): \n",
    "            reward_sim = self.run_n_episodes(strategy, n_episodes, verbose, strategy_args)\n",
    "            self.reward_sims[i_sims] = reward_sim\n",
    "        print(f\"Average reward per sim is {self.reward_sims.mean()} with sd {self.reward_sims.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def random_uniform(self: BanditGame):\n",
    "    return self.i_episode % self.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/jonas/mambaforge/envs/MMDS/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "169.35060480902462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = BanditGame()\n",
    "bg.run_n_episodes(bg.random_uniform, 1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per sim is -16.87930658481435 with sd 317.9285680140441\n"
     ]
    }
   ],
   "source": [
    "bg.run_n_sims(bg.random_uniform, 1000, 1000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def explore_then_exploit(self: BanditGame, exploration_factor: float):\n",
    "    assert 0 <= exploration_factor and exploration_factor <= 1\n",
    "    n_exploration_eps = self.n_episodes * exploration_factor\n",
    "    if self.i_episode <= n_exploration_eps:\n",
    "        return self.i_episode % self.env.action_space.n\n",
    "    else:\n",
    "        return self.average_rewards.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241.7556901403777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg.run_n_episodes(bg.explore_then_exploit, 1000, False, [0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per sim is 1012.0732466514124 with sd 583.3513174223067\n"
     ]
    }
   ],
   "source": [
    "bg.run_n_sims(bg.explore_then_exploit, 1000, 1000, False, [rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.0\n",
      "Average reward per sim is 1037.462754173668 with sd 604.975693308411\n",
      "rate: 0.01\n",
      "Average reward per sim is 1225.074402403252 with sd 627.9652149188141\n",
      "rate: 0.02\n",
      "Average reward per sim is 1322.9014369669349 with sd 660.1153122602763\n",
      "rate: 0.03\n",
      "Average reward per sim is 1344.6597638918759 with sd 630.6822931766918\n",
      "rate: 0.04\n",
      "Average reward per sim is 1380.5978193716016 with sd 630.4201131197293\n",
      "rate: 0.05\n",
      "Average reward per sim is 1363.3525800380448 with sd 607.5578162951064\n",
      "rate: 0.06\n",
      "Average reward per sim is 1323.1003628029805 with sd 591.5058546184163\n",
      "rate: 0.07\n",
      "Average reward per sim is 1318.9621005161903 with sd 580.4700180824374\n",
      "rate: 0.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rate \u001b[39m=\u001b[39m i \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mrate:\u001b[39m\u001b[39m'\u001b[39m, rate)\n\u001b[0;32m----> 4\u001b[0m bg\u001b[39m.\u001b[39;49mrun_n_sims(bg\u001b[39m.\u001b[39;49mexplore_then_exploit, \u001b[39m1000\u001b[39;49m, \u001b[39m1000\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m, [rate])\n",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m, in \u001b[0;36mBanditGame.run_n_sims\u001b[0;34m(self, strategy, n_sims, n_episodes, verbose, strategy_args)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# make strategies comparable\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# np.random.seed(42)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# self.env.seed(34)  \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i_sims \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_sims): \n\u001b[0;32m---> 48\u001b[0m     reward_sim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_n_episodes(strategy, n_episodes, verbose, strategy_args)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_sims[i_sims] \u001b[39m=\u001b[39m reward_sim\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage reward per sim is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_sims\u001b[39m.\u001b[39mmean()\u001b[39m}\u001b[39;00m\u001b[39m with sd \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_sims\u001b[39m.\u001b[39mstd()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mBanditGame.run_n_episodes\u001b[0;34m(self, strategy, n_episodes, verbose, strategy_args)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepisode Number is\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi_episode)\n\u001b[0;32m---> 22\u001b[0m action \u001b[39m=\u001b[39m strategy(\u001b[39m*\u001b[39;49mstrategy_args)\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maction is\u001b[39m\u001b[39m\"\u001b[39m, action)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mexplore_then_exploit\u001b[0;34m(self, exploration_factor)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi_episode \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m      7\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage_rewards\u001b[39m.\u001b[39margmax()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, 15): # hyperparameter optimization\n",
    "    rate = i / 100\n",
    "    print('rate:', rate)\n",
    "    bg.run_n_sims(bg.explore_then_exploit, 1000, 1000, False, [rate])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- how many rounds exploring is optimal?\n",
    "- after a little exploration we should focus more onto the more promising ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie695_reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8669abbd5b69a0f56090f9a5500c0456ddc4763ccf103321153544a4b7c23887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
